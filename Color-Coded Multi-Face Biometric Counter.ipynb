{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c488aa",
   "metadata": {},
   "source": [
    "### **Computer Vision Multi-Face Blink & Yawn Counter using Color-Coded MediaPipe Mesh.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8037585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "import time\n",
    "\n",
    "# CONFIGURATION\n",
    "EAR_THRESHOLD = 0.21     # Eye closed threshold\n",
    "MAR_THRESHOLD = 0.09     # Mouth open threshold\n",
    "\n",
    "Right_Eye = [33, 160, 158, 133, 153, 144]\n",
    "Left_Eye  = [362, 385, 387, 263, 373, 380]\n",
    "\n",
    "Lip_Points = [13, 14]    # Upper + lower lip points\n",
    "\n",
    "# Text properties (GLOBAL â†’ avoids UnboundLocalError)\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_SCALE = 0.6\n",
    "THICKNESS = 2\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "def ear(pts):\n",
    "    A = dist.euclidean(pts[1], pts[5])\n",
    "    B = dist.euclidean(pts[2], pts[4])\n",
    "    C = dist.euclidean(pts[0], pts[3])\n",
    "    return (A + B) / (2.0 * C) if C != 0 else 0.0\n",
    "\n",
    "\n",
    "def normalized_lip_distance(u, l):\n",
    "    u = np.array([u.x, u.y])\n",
    "    l = np.array([l.x, l.y])\n",
    "    return np.linalg.norm(u - l)\n",
    "\n",
    "\n",
    "def pixel_landmarks(face, w, h):\n",
    "    return [(int(lm.x * w), int(lm.y * h)) for lm in face.landmark]\n",
    "\n",
    "\n",
    "def face_box(face, w, h):\n",
    "    xs = [lm.x for lm in face.landmark]\n",
    "    ys = [lm.y for lm in face.landmark]\n",
    "    return int(min(xs)*w), int(min(ys)*h), int(max(xs)*w), int(max(ys)*h)\n",
    "\n",
    "# MAIN FACE DETECTION FUNCTION\n",
    "def start_detection():\n",
    "\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    mp_draw = mp.solutions.drawing_utils\n",
    "    draw_spec = mp_draw.DrawingSpec(color=(0, 255, 255), thickness=1, circle_radius=1)\n",
    "\n",
    "    counters = {}     # Per face blink/mouth count\n",
    "    p_time = 0        # For FPS\n",
    "\n",
    "    cv2.namedWindow(\"Multi-Face Mesh EAR + MAR\", cv2.WINDOW_NORMAL)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=10,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.7,\n",
    "        min_tracking_confidence=0.7\n",
    "    ) as mesh:\n",
    "\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                continue\n",
    "\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            h, w, _ = frame.shape\n",
    "\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = mesh.process(rgb)\n",
    "\n",
    "            # FACE PROCESSING\n",
    "            if results.multi_face_landmarks:\n",
    "                for face_id, face in enumerate(results.multi_face_landmarks):\n",
    "\n",
    "                    # Draw full mesh\n",
    "                    mp_draw.draw_landmarks(\n",
    "                        frame,\n",
    "                        face,\n",
    "                        mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                        draw_spec,\n",
    "                        draw_spec\n",
    "                    )\n",
    "\n",
    "                    # Create counter bucket for NEW face_id\n",
    "                    if face_id not in counters:\n",
    "                        counters[face_id] = {\n",
    "                            \"L\": 0, \"R\": 0, \"M\": 0,\n",
    "                            \"prev_L\": 1, \"prev_R\": 1, \"prev_M\": 0\n",
    "                        }\n",
    "\n",
    "                    pts = pixel_landmarks(face, w, h)\n",
    "                    x1, y1, x2, y2 = face_box(face, w, h)\n",
    "\n",
    "                    # -------- EYE EAR --------\n",
    "                    RE = [pts[i] for i in Right_Eye]\n",
    "                    LE = [pts[i] for i in Left_Eye]\n",
    "\n",
    "                    rEAR = ear(RE)\n",
    "                    lEAR = ear(LE)\n",
    "\n",
    "                    # Left blink\n",
    "                    if lEAR < EAR_THRESHOLD and counters[face_id][\"prev_L\"] == 1:\n",
    "                        counters[face_id][\"L\"] += 1\n",
    "\n",
    "                    # Right blink\n",
    "                    if rEAR < EAR_THRESHOLD and counters[face_id][\"prev_R\"] == 1:\n",
    "                        counters[face_id][\"R\"] += 1\n",
    "\n",
    "                    counters[face_id][\"prev_L\"] = 1 if lEAR > EAR_THRESHOLD else 0\n",
    "                    counters[face_id][\"prev_R\"] = 1 if rEAR > EAR_THRESHOLD else 0\n",
    "\n",
    "                    # -------- MOUTH MAR --------\n",
    "                    upper = face.landmark[Lip_Points[0]]\n",
    "                    lower = face.landmark[Lip_Points[1]]\n",
    "                    mar = normalized_lip_distance(upper, lower)\n",
    "\n",
    "                    if mar > MAR_THRESHOLD and counters[face_id][\"prev_M\"] == 0:\n",
    "                        counters[face_id][\"M\"] += 1\n",
    "                        counters[face_id][\"prev_M\"] = 1\n",
    "\n",
    "                    if mar <= MAR_THRESHOLD:\n",
    "                        counters[face_id][\"prev_M\"] = 0\n",
    "\n",
    "                    # -------- DRAW POINTS --------\n",
    "                    for p in RE + LE:\n",
    "                        cv2.circle(frame, p, 2, (0, 0, 255), -1)\n",
    "\n",
    "                    cv2.circle(frame, pts[Lip_Points[0]], 3, (255, 100, 0), -1)\n",
    "                    cv2.circle(frame, pts[Lip_Points[1]], 3, (255, 100, 0), -1)\n",
    "\n",
    "                    # -------- TEXT OUTPUT --------\n",
    "                    cv2.putText(frame,\n",
    "                                f\"FACE {face_id} - Avg EAR: {((rEAR + lEAR) / 2):.2f}\",\n",
    "                                (x1, y1 - 10),\n",
    "                                FONT, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "                    cv2.putText(frame,\n",
    "                                f\"L-Blinks: {counters[face_id]['L']} / R-Blinks: {counters[face_id]['R']}\",\n",
    "                                (x1, y2 + 20),\n",
    "                                FONT, FONT_SCALE, (0, 0, 255), THICKNESS)\n",
    "\n",
    "                    cv2.putText(frame,\n",
    "                                f\"Mouth Opens: {counters[face_id]['M']} (MAR: {mar:.2f})\",\n",
    "                                (x1, y2 + 45),\n",
    "                                FONT, FONT_SCALE, (0, 255, 255), THICKNESS)\n",
    "            # FPS DISPLAY\n",
    "            c_time = time.time()\n",
    "            fps = 1 / (c_time - p_time) if c_time != p_time else 0\n",
    "            p_time = c_time\n",
    "\n",
    "            cv2.putText(frame, f\"FPS: {int(fps)}\",\n",
    "                        (10, 30), FONT, 0.7, (255, 0, 0), 2)\n",
    "\n",
    "            cv2.imshow(\"Multi-Face Mesh EAR + MAR\", frame)\n",
    "\n",
    "            if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# RUN\n",
    "if __name__ == \"__main__\":\n",
    "    start_detection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381bada2",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Features Included in the Code\n",
    "\n",
    "\n",
    "### 1. **Multi-Face Detection and Tracking** \n",
    "\n",
    "* **Description:** The system uses `mp.solutions.face_mesh` with `max_num_faces=10` to simultaneously detect and process up to ten faces in the video feed. It uses a dictionary (`counters`) indexed by `face_id` to **independently track events** (blinks, mouth opens) for each person, maintaining persistence across frames.\n",
    "\n",
    "### 2. **Real-Time Blink Detection (Eye Aspect Ratio - EAR)**\n",
    "\n",
    "* **Description:** It calculates the **Eye Aspect Ratio (EAR)** using six specific landmarks for both the right and left eyes. The EAR, calculated by the `ear(pts)` helper function, provides a numerical measure of eye openness.\n",
    "* **Counting Accuracy:** The core counting logic uses a **falling edge detection** (`lEAR < EAR_THRESHOLD and counters[face_id][\"prev_L\"] == 1`) to accurately count a blink event only when the eye transitions from an **open state** to a **closed state**, ensuring a single count per full blink.\n",
    "\n",
    "### 3. **Real-Time Mouth Open/Yawn Detection (Normalized Lip Distance - MAR)** \n",
    "\n",
    "* **Description:** It calculates the vertical distance between two key lip landmarks (13 and 14) in **normalized coordinates** (renamed to **MAR** in the code, standing for *Mouth Aspect Ratio* or distance).\n",
    "* **Counting Accuracy:** The counting logic uses a **rising edge detection** (`mar > MAR_THRESHOLD and counters[face_id][\"prev_M\"] == 0`) to accurately count an opening event (yawn, speaking, etc.) only when the mouth transitions from a **closed state** to an **open state**, ensuring a single count per event.\n",
    "\n",
    "### 4. **Multi-Color Face Mesh Visualization** \n",
    "\n",
    "* **Description:** This feature replaces a simple bounding box with the detailed **MediaPipe Face Mesh** using `mp_drawing.draw_landmarks`. Crucially, it assigns a **unique color** from the `FACE_COLORS` list to the mesh lines and points of **each detected face** (`current_face_color = FACE_COLORS[face_id % len(FACE_COLORS)]`). This visually separates and tracks individuals in the frame.\n",
    "\n",
    "### 5. **Enhanced Text Display and Information Overlay** \n",
    "\n",
    "* **Description:** It uses `cv2.putText` to overlay event counts and metrics on the video feed. The display is designed for clarity, grouping related statistics and placing them near the respective face's bounding box.\n",
    "    * **Face-Specific Metrics:** Displays the `FACE ID` and average **EAR value** (e.g., `Avg EAR: 0.28`).\n",
    "    * **Grouped Counts:** Consolidates left/right blink counts and mouth open counts into separate, clear lines, and uses the face's **unique color** for the text associated with that face.\n",
    "\n",
    "### 6. **Real-Time Frame Rate (FPS) Monitor** \n",
    "\n",
    "* **Description:** A simple performance monitoring feature that calculates and displays the **Frames Per Second (FPS)** in the top-left corner. It measures the time difference between consecutive frames using the `time` library, providing immediate feedback on the application's processing speed.\n",
    "\n",
    "### 7. **Video Handling and Preprocessing** \n",
    "\n",
    "* **Description:** Includes standard video capture and manipulation:\n",
    "    * **Camera Initialization:** Uses `cv2.VideoCapture(0)` to access the primary webcam.\n",
    "    * **Mirroring:** Uses `cv2.flip(frame, 1)` to flip the image horizontally, providing a mirrored view that is more intuitive for the user.\n",
    "    * **Color Conversion:** Converts the frame from BGR (OpenCV default) to RGB for processing by the MediaPipe model (`cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Media_Pipe (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
